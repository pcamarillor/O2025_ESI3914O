{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01ec775-b46b-4286-a311-95a3d26fd924",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 05**: Data pipeline with Neo4j\n",
    "\n",
    "**Date**: October 2nd 2025\n",
    "\n",
    "**Student Name**: Arantxa Angulo\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893c817",
   "metadata": {},
   "source": [
    "# Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6ad01-1a8b-4979-8a0c-fb00c457b53b",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "On the data team at Stack Overflow, we spend a lot of time and energy thinking about tech ecosystems and how technologies are related to each other. One way to get at this idea of relationships between technologies is tag correlations, how often technology tags at Stack Overflow appear together relative to how often they appear separately. One place we see developers using tags at Stack Overflow is on their Developer Stories, or professional profiles/CVs/resumes. If we are interested in how technologies are connected and how they are used together, developers' own descriptions of their work and careers is a great place to get that.\n",
    "\n",
    "**Content**\n",
    "\n",
    "A network of technology tags from Developer Stories on the Stack Overflow online developer community website.\n",
    "\n",
    "This is organized as two tables:\n",
    "\n",
    "* stack_network_links contains links of the network, the source and target tech tags plus the value of the the link between each pair\n",
    "* stack_network_nodes contains nodes of the network, the name of each node, which group that node belongs to (calculated via a cluster walktrap), and a node size based on how often that technology tag is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e84c3",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed17a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-65212807-7116-4da5-841b-f13676ecc3e0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#caniuse-core;1.3.0 in central\n",
      "\tfound org.neo4j#caniuse-api;1.3.0 in central\n",
      "\tfound org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in central\n",
      "\tfound org.jetbrains#annotations;13.0 in central\n",
      "\tfound org.neo4j#caniuse-neo4j-detection;1.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound io.netty#netty-handler;4.1.127.Final in central\n",
      "\tfound io.netty#netty-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.127.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.127.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.73.Final in central\n",
      "\tfound io.projectreactor#reactor-core;3.6.11 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in central\n",
      "\tfound org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in central\n",
      ":: resolution report :: resolve 974ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\tio.netty#netty-buffer;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.73.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.127.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.6.11 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.jetbrains#annotations;13.0 from central in [default]\n",
      "\torg.jetbrains.kotlin#kotlin-stdlib;2.1.20 from central in [default]\n",
      "\torg.neo4j#caniuse-api;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-core;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-neo4j-detection;1.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver-slim;4.4.21 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   0   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-65212807-7116-4da5-841b-f13676ecc3e0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/19ms)\n",
      "25/10/23 17:06:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on SparkSQL\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.13:5.3.10_for_spark_3\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93cd621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+\n",
      "|name     |group|nodesize|\n",
      "+---------+-----+--------+\n",
      "|html     |6    |272.45  |\n",
      "|css      |6    |341.17  |\n",
      "|hibernate|8    |29.83   |\n",
      "|spring   |8    |52.84   |\n",
      "|ruby     |3    |70.14   |\n",
      "+---------+-----+--------+\n",
      "only showing top 5 rows\n",
      "Links DataFrame:\n",
      "+----------------+------+---------+\n",
      "|source          |target|value    |\n",
      "+----------------+------+---------+\n",
      "|azure           |.net  |20.933193|\n",
      "|sql-server      |.net  |32.322525|\n",
      "|asp.net         |.net  |48.40703 |\n",
      "|entity-framework|.net  |24.370903|\n",
      "|wpf             |.net  |32.350925|\n",
      "+----------------+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Build schema\n",
    "# Import your module\n",
    "from arantxa.spark_utils import SparkUtils\n",
    "\n",
    "nodes_schema = SparkUtils.generate_schema([\n",
    "    (\"name\", \"string\"),\n",
    "    (\"group\", \"int\"),\n",
    "    (\"nodesize\", \"float\")\n",
    "])\n",
    "\n",
    "# Define schema for links/edges\n",
    "links_schema = SparkUtils.generate_schema([\n",
    "    (\"source\", \"string\"),\n",
    "    (\"target\", \"string\"),\n",
    "    (\"value\", \"float\")\n",
    "])\n",
    "\n",
    "# Read the data\n",
    "df_nodes = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(nodes_schema) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/stack_network_nodes.csv\")\n",
    "\n",
    "df_links = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(links_schema) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/stack_network_links.csv\")\n",
    "\n",
    "print(\"Nodes DataFrame:\")\n",
    "df_nodes.show(5, truncate=False)\n",
    "\n",
    "print(\"Links DataFrame:\")\n",
    "df_links.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e82d5",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000b6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech Nodes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|id      |group|size |\n",
      "+--------+-----+-----+\n",
      "|qt      |1    |10.53|\n",
      "|iphone  |4    |15.29|\n",
      "|unix    |5    |15.67|\n",
      "|devops  |9    |9.81 |\n",
      "|embedded|1    |13.27|\n",
      "+--------+-----+-----+\n",
      "only showing top 5 rows\n",
      "Tech Edges:\n",
      "+-------------+----------------+---------+\n",
      "|src          |dst             |weight   |\n",
      "+-------------+----------------+---------+\n",
      "|sql          |asp.net         |21.672264|\n",
      "|wpf          |entity-framework|24.2282  |\n",
      "|c            |python          |22.320433|\n",
      "|python       |r               |28.535748|\n",
      "|ruby-on-rails|ruby            |95.36131 |\n",
      "+-------------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "Total nodes: 115\n",
      "Total edges: 490\n"
     ]
    }
   ],
   "source": [
    "# Add the code for your transformations to create nodes and edges DataFrames HERE\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create nodes DataFrame\n",
    "tech_nodes = df_nodes.select(\n",
    "    col(\"name\").alias(\"id\"),      # Node identifier\n",
    "    col(\"group\"),                # Category group\n",
    "    col(\"nodesize\").alias(\"size\") # Relative size\n",
    ").dropDuplicates([\"id\"])         # Remove duplicates\n",
    "\n",
    "print(\"Tech Nodes:\")\n",
    "tech_nodes.show(5, truncate=False)\n",
    "\n",
    "# Create edges DataFrame\n",
    "tech_edges = df_links.select(\n",
    "    col(\"source\").alias(\"src\"),   # Source node\n",
    "    col(\"target\").alias(\"dst\"),   # Target node  \n",
    "    col(\"value\").alias(\"weight\")  # Relationship weight\n",
    ").dropDuplicates()               # Remove duplicates\n",
    "\n",
    "print(\"Tech Edges:\")\n",
    "tech_edges.show(5, truncate=False)\n",
    "\n",
    "print(f\"Total nodes: {tech_nodes.count()}\")\n",
    "print(f\"Total edges: {tech_edges.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a68775",
   "metadata": {},
   "source": [
    "# Writing Data in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01d7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 tech nodes written to Neo4j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 CO_OCCURS relationships written to Neo4j\n"
     ]
    }
   ],
   "source": [
    "# Add the code to write a graph from PySpark's DataFrames to Neo4j\n",
    "# Writing Data in Neo4j\n",
    "neo4j_url = \"bolt://neo4j-iteso:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_passwd = \"neo4j@1234\"\n",
    "\n",
    "# Write nodes to Neo4j\n",
    "tech_nodes.write \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"labels\", \":Tech\") \\\n",
    "    .option(\"node.keys\", \"id\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"{tech_nodes.count()} tech nodes written to Neo4j\")\n",
    "\n",
    "# Write edges to Neo4j\n",
    "tech_edges.write \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"relationship\", \"CO_OCCURS\") \\\n",
    "    .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "    .option(\"relationship.source.labels\", \":Tech\") \\\n",
    "    .option(\"relationship.source.save.mode\", \"match\") \\\n",
    "    .option(\"relationship.source.node.keys\", \"src:id\") \\\n",
    "    .option(\"relationship.target.labels\", \":Tech\") \\\n",
    "    .option(\"relationship.target.save.mode\", \"match\") \\\n",
    "    .option(\"relationship.target.node.keys\", \"dst:id\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"{tech_edges.count()} CO_OCCURS relationships written to Neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30287ea",
   "metadata": {},
   "source": [
    "# Read and Query Graphs with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03ca7b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology tags in Neo4j (first 10):\n",
      "+-------------------+\n",
      "|technology_name    |\n",
      "+-------------------+\n",
      "|.net               |\n",
      "|agile              |\n",
      "|ajax               |\n",
      "|amazon-web-services|\n",
      "|android            |\n",
      "|android-studio     |\n",
      "|angular            |\n",
      "|angular2           |\n",
      "|angularjs          |\n",
      "|apache             |\n",
      "+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Add the code to read a data frame from Neo4J and run a simple query to verify \n",
    "\n",
    "# Simple query: Get all technology tags\n",
    "tech_tags = spark.read \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"query\", \"\"\"\n",
    "        MATCH (t:Tech)\n",
    "        RETURN t.id AS technology_name\n",
    "        ORDER BY t.id\n",
    "    \"\"\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Technology tags in Neo4j (first 10):\")\n",
    "tech_tags.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9078a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b6208-60d1-4796-9281-45a2f891fdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
