{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 05**: Data pipeline with Neo4j\n",
    "\n",
    "**Date**: October 2nd 2025\n",
    "\n",
    "**Student Name**: Luis Daniel Arellano Núñez\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893c817",
   "metadata": {},
   "source": [
    "# Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57aac7-9a13-4a5c-8370-fa618e8eb1f5",
   "metadata": {},
   "source": [
    "This dataset contains information about a github based social netowkr with nodes that are developers who have starred at least 10 repositories and edges are mutual follower relationships between them\n",
    "\n",
    "You can find this dataset in this url:\n",
    "https://www.kaggle.com/datasets/kausthubkannan/github-social-network/data\n",
    "\n",
    "You only need to download the 2 csv and save them in the correspondig path in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e84c3",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed17a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-632e73b1-a49c-490f-a418-cf2cfacc26a3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 in central\n",
      "\tfound org.neo4j#caniuse-core;1.3.0 in central\n",
      "\tfound org.neo4j#caniuse-api;1.3.0 in central\n",
      "\tfound org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in central\n",
      "\tfound org.jetbrains#annotations;13.0 in central\n",
      "\tfound org.neo4j#caniuse-neo4j-detection;1.3.0 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound io.netty#netty-handler;4.1.127.Final in central\n",
      "\tfound io.netty#netty-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.127.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.127.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.127.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.127.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.73.Final in central\n",
      "\tfound io.projectreactor#reactor-core;3.6.11 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in central\n",
      "\tfound org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in central\n",
      ":: resolution report :: resolve 634ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tio.netty#netty-buffer;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.73.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.127.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.127.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.6.11 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.jetbrains#annotations;13.0 from central in [default]\n",
      "\torg.jetbrains.kotlin#kotlin-stdlib;2.1.20 from central in [default]\n",
      "\torg.neo4j#caniuse-api;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-core;1.3.0 from central in [default]\n",
      "\torg.neo4j#caniuse-neo4j-detection;1.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver-slim;4.4.21 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   0   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-632e73b1-a49c-490f-a418-cf2cfacc26a3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/11ms)\n",
      "25/10/03 18:28:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on SparkSQL\") \\\n",
    "    .master(\"spark://8c99a6c586f5:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.13:5.3.10_for_spark_3\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a93cd621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|id |name        |ml_target|\n",
      "+---+------------+---------+\n",
      "|0  |Eiryyy      |0        |\n",
      "|1  |shawflying  |0        |\n",
      "|2  |JpMCarrilho |1        |\n",
      "|3  |SuhwanCha   |0        |\n",
      "|4  |sunilangadi2|1        |\n",
      "+---+------------+---------+\n",
      "only showing top 5 rows\n",
      "+----+-----+\n",
      "|id_1|id_2 |\n",
      "+----+-----+\n",
      "|0   |23977|\n",
      "|1   |34526|\n",
      "|1   |2370 |\n",
      "|1   |14683|\n",
      "|1   |29982|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Build schema\n",
    "# Import your module\n",
    "from Daniel_Arellano.sql_im import SparkUtils\n",
    "from pyspark.sql.functions import regexp_replace, split, col\n",
    "\n",
    "users_schema = SparkUtils.generate_schema([\n",
    "    (\"id\", \"int\"),\n",
    "    (\"name\", \"string\"),\n",
    "    (\"ml_target\", \"int\")\n",
    "])\n",
    "\n",
    "df_users = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(users_schema) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/lab6/nodes\")\n",
    "\n",
    "df_users.show(5, truncate=False)\n",
    "\n",
    "edges_schema = SparkUtils.generate_schema([\n",
    "    (\"id_1\", \"int\"),\n",
    "    (\"id_2\", \"int\")\n",
    "])\n",
    "\n",
    "df_edges = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(edges_schema) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/lab6/edges\")\n",
    "df_edges.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e82d5",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "000b6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|id |name        |ml_target|\n",
      "+---+------------+---------+\n",
      "|0  |Eiryyy      |0        |\n",
      "|1  |shawflying  |0        |\n",
      "|2  |JpMCarrilho |1        |\n",
      "|3  |SuhwanCha   |0        |\n",
      "|4  |sunilangadi2|1        |\n",
      "+---+------------+---------+\n",
      "only showing top 5 rows\n",
      "+---+-----+\n",
      "|src|dst  |\n",
      "+---+-----+\n",
      "|0  |23977|\n",
      "|1  |34526|\n",
      "|1  |2370 |\n",
      "|1  |14683|\n",
      "|1  |29982|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add the code for your transformations to create nodes and edges DataFrames HERE\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Artists Nodes\n",
    "user_nodes = df_users.select(\n",
    "    col(\"id\").alias(\"id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"ml_target\")\n",
    ").dropDuplicates([\"id\"])\n",
    "user_nodes.show(5, truncate=False)\n",
    "\n",
    "# --- EDGES ---\n",
    "\n",
    "# Collaborations: relationships between User -> User\n",
    "collab_edges = df_edges.select(\n",
    "    col(\"id_1\").alias(\"src\"),   # source node (User 1)\n",
    "    col(\"id_2\").alias(\"dst\"),   # destination node (User 2)\n",
    ")\n",
    "collab_edges.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a68775",
   "metadata": {},
   "source": [
    "# Writing Data in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01d7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37700 users wrote in Neo4j\n"
     ]
    }
   ],
   "source": [
    "# Add the code to write a graph from PySpark's DataFrames to Neo4j\n",
    "neo4j_url = \"bolt://neo4j-iteso:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_passwd = \"neo4j@1234\"\n",
    "\n",
    "user_nodes.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"labels\", \":User\") \\\n",
    "  .option(\"node.keys\", \"id\") \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{user_nodes.count()} users wrote in Neo4j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "980c422c-87a5-48e3-9501-d12a82a64ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 follows edges wrote in Neo4j\n"
     ]
    }
   ],
   "source": [
    "collab_edges.write \\\n",
    "  .format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"Overwrite\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "  .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "  .option(\"relationship\", \"FOLLOWS\") \\\n",
    "  .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "  .option(\"relationship.source.labels\", \":User\") \\\n",
    "  .option(\"relationship.source.save.mode\", \"match\") \\\n",
    "  .option(\"relationship.source.node.keys\", \"src:id\") \\\n",
    "  .option(\"relationship.target.labels\", \":User\") \\\n",
    "  .option(\"relationship.target.save.mode\", \"match\") \\\n",
    "  .option(\"relationship.target.node.keys\", \"dst:id\") \\\n",
    "  .option(\"batch.size\", 500) \\\n",
    "  .save()\n",
    "\n",
    "print(f\"{collab_edges.count()} follows edges wrote in Neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30287ea",
   "metadata": {},
   "source": [
    "# Read and Query Graphs with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ca7b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------+\n",
      "|origin_user |dest_user    |ml_target|\n",
      "+------------+-------------+---------+\n",
      "|danalex97   |benjaminaaron|1        |\n",
      "|danalex97   |panpan2      |1        |\n",
      "|sunilangadi2|mshahriarinia|1        |\n",
      "|sunilangadi2|KnightBaron  |1        |\n",
      "|Daichou     |penk         |1        |\n",
      "+------------+-------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add the code to read a data frame from Neo4J and run a simple query to verify \n",
    "\n",
    "cypher_df = spark.read \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option(\"authentication.basic.username\", neo4j_user) \\\n",
    "    .option(\"authentication.basic.password\", neo4j_passwd) \\\n",
    "    .option(\"query\",\n",
    "            \"\"\"\n",
    "            MATCH (u:User)-[r:FOLLOWS]->(u2:User)\n",
    "            WHERE coalesce(u.ml_target, 0) > 0\n",
    "            WITH u, u2, u.ml_target AS ml_target\n",
    "            LIMIT 20\n",
    "            RETURN u.name AS origin_user, u2.name AS dest_user, ml_target\n",
    "            \"\"\") \\\n",
    "    .load()\n",
    "\n",
    "cypher_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9078a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
