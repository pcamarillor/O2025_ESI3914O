{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38eb160f-dd91-4bdd-84b4-23bde49a6ec3",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 07**: Structured Streaming with Files\n",
    "\n",
    "**Date**: October 7th 2025\n",
    "\n",
    "**Student Name**: Renata Tejeda\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1667b495-5893-49a1-88e3-4209b3e92d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/10 03:03:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Examples on Structured Streaming\")\n",
    "      .master(\"spark://34c8a8d7a9e7:7077\")   \n",
    "      .config(\"spark.ui.port\", \"4040\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")                     \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf8d353-cfb2-44e4-b58b-e67dd2cfa99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR  = /opt/spark/work-dir/labs/lab07/input_dir\n",
      "CHECKPOINT = /opt/spark/work-dir/labs/lab07/chk_basic\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "LAB07_DIR = Path.cwd().resolve()\n",
    "INPUT_DIR  = str((LAB07_DIR / \"input_dir\").resolve())\n",
    "CHECKPOINT = str((LAB07_DIR / \"chk_basic\").resolve())\n",
    "\n",
    "(Path(INPUT_DIR)).mkdir(parents=True, exist_ok=True)\n",
    "(Path(CHECKPOINT)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"INPUT_DIR  =\", INPUT_DIR)\n",
    "print(\"CHECKPOINT =\", CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b7ae81-d1cf-4d5a-9f2e-31fbc1fc30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si funciono\n"
     ]
    }
   ],
   "source": [
    "from codrenatat.spark_utils import SparkUtils\n",
    "\n",
    "log_schema_columns = [\n",
    "    (\"event_time\", \"string\"),\n",
    "    (\"request_id\", \"string\"),\n",
    "    (\"ip\", \"string\"),\n",
    "    (\"method\", \"string\"),\n",
    "    (\"path\", \"string\"),\n",
    "    (\"status\", \"int\"),\n",
    "    (\"bytes\", \"int\"),\n",
    "    (\"latency_ms\", \"int\"),\n",
    "    (\"user_agent\", \"string\")\n",
    "]\n",
    "\n",
    "LOG_SCHEMA = SparkUtils.generate_schema(log_schema_columns)\n",
    "\n",
    "print(\"Si funciono\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece481c1-b988-4ad2-81e1-046c6fa88c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- request_id: string (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      " |-- latency_ms: integer (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "logs = (\n",
    "    spark.readStream\n",
    "         .format(\"json\")\n",
    "         .schema(LOG_SCHEMA)                 \n",
    "         .option(\"path\", INPUT_DIR)          \n",
    "         .option(\"maxFilesPerTrigger\", 1)    \n",
    "         .load()\n",
    ")\n",
    "\n",
    "logs.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754923e5-3249-4e9a-8407-4c9ae81464c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count as _count\n",
    "\n",
    "err500_by_path = (\n",
    "    logs.where(col(\"status\") == 500)\n",
    "        .groupBy(col(\"path\"))\n",
    "        .agg(_count(\"*\").alias(\"count_500\"))\n",
    ")\n",
    "\n",
    "ALERT_THRESHOLD = 3\n",
    "alerts = err500_by_path.where(col(\"count_500\") >= ALERT_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783f48cd-a0d8-43a7-90a2-c1177b51c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming started. Leave this running.\n"
     ]
    }
   ],
   "source": [
    "q = (\n",
    "    alerts.writeStream\n",
    "          .format(\"console\")\n",
    "          .outputMode(\"update\")            \n",
    "          .option(\"truncate\", \"false\")\n",
    "          .option(\"numRows\", 50)\n",
    "          .option(\"checkpointLocation\", CHECKPOINT)\n",
    "          .start()\n",
    ")\n",
    "print(\"Streaming started. Leave this running.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b17952-62c5-47ce-a9b9-54e99d161083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /opt/spark/work-dir/labs/lab07\n",
      "Productor = /opt/spark/work-dir/lib/codrenatat/log_file_producer.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------+---------+\n",
      "|path   |count_500|\n",
      "+-------+---------+\n",
      "|/orders|4        |\n",
      "|/login |6        |\n",
      "|/cart  |6        |\n",
      "+-------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------------+---------+\n",
      "|path         |count_500|\n",
      "+-------------+---------+\n",
      "|/api/v1/items|4        |\n",
      "|/            |11       |\n",
      "|/api/v1/pay  |12       |\n",
      "+-------------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-----------+---------+\n",
      "|path       |count_500|\n",
      "+-----------+---------+\n",
      "|/login     |7        |\n",
      "|/          |12       |\n",
      "|/api/v1/pay|13       |\n",
      "+-----------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------+---------+\n",
      "|path  |count_500|\n",
      "+------+---------+\n",
      "|/login|8        |\n",
      "|/     |13       |\n",
      "|/cart |7        |\n",
      "+------+---------+\n",
      "\n",
      "[producer] writing 5 files to: /opt/spark/work-dir/labs/lab07/input_dir\n",
      "[producer] wrote 1/5: logs_20251010T030409_9409.json\n",
      "[producer] wrote 2/5: logs_20251010T030412_9539.json\n",
      "[producer] wrote 3/5: logs_20251010T030415_6211.json\n",
      "[producer] wrote 4/5: logs_20251010T030418_3870.json\n",
      "[producer] wrote 5/5: logs_20251010T030421_2214.json\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------+---------+\n",
      "|path   |count_500|\n",
      "+-------+---------+\n",
      "|/orders|6        |\n",
      "|/      |16       |\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, os\n",
    "from pathlib import Path\n",
    "\n",
    "candidatos = [\n",
    "    Path(\"../../lib/codrenatat/log_file_producer.py\").resolve(),\n",
    "    Path(\"../../../../lib/codrenatat/log_file_producer.py\").resolve(),\n",
    "]\n",
    "\n",
    "productor = None\n",
    "for p in candidatos:\n",
    "    if p.exists():\n",
    "        productor = p\n",
    "        break\n",
    "\n",
    "print(\"CWD =\", os.getcwd())\n",
    "print(\"Productor =\", productor)\n",
    "\n",
    "if productor is None:\n",
    "    print(\"No encontré el script.\")\n",
    "else:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable, str(productor),\n",
    "            \"--output\", INPUT_DIR,   \n",
    "            \"--rows\", \"80\",\n",
    "            \"--files\", \"5\",          \n",
    "            \"--interval\", \"3\",\n",
    "        ],\n",
    "        check=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882f1831-7ed4-42b6-9708-e665d456652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in input_dir (15):\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025139_1990.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025142_8512.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025145_1850.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025148_4330.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025151_5629.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025737_8261.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025740_6900.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025743_6396.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025746_6524.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T025749_4445.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T030409_9409.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T030412_9539.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T030415_6211.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T030418_3870.json\n",
      " - /opt/spark/work-dir/labs/lab07/input_dir/logs_20251010T030421_2214.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "parts = sorted(str(p) for p in (Path(INPUT_DIR)).glob(\"*.json\"))\n",
    "print(f\"Files in input_dir ({len(parts)}):\")\n",
    "for p in parts:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183f1ccd-ffd3-4e88-af04-36af225e1970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream stopped\n"
     ]
    }
   ],
   "source": [
    "q.stop()\n",
    "print(\"stream stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67133c6e-f28c-4ba1-8fa1-417243aaabc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
