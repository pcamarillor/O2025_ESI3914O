{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "---\n",
    "\n",
    "**Lab 03**: Data Cleaning and Transformation Pipeline\n",
    "\n",
    "**Date**: September 18th 2025\n",
    "\n",
    "**Student Name**: Francisco Delgado\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546462aa-00c7-418b-ab00-6794b28b6e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59ed86-8fb9-47b5-a114-ee3b99adbd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Examples on data sources (Files)\") \\\n",
    "    .master(\"spark://c88eb4f6e4be:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896841f8-a1bd-4abe-b7c0-6b279a58667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcamarillor.spark_utils import SparkUtils\n",
    "airlines_schema_columns = [(\"index\", \"int\"), \n",
    "     (\"airline\", \"string\"), \n",
    "     (\"flight\", \"string\"),\n",
    "     (\"source_city\", \"string\"),\n",
    "     (\"departure_time\", \"string\"),\n",
    "     (\"stops\", \"string\"),\n",
    "     (\"arrival_time\", \"string\"),\n",
    "     (\"destination_city\", \"string\"),\n",
    "     (\"class\", \"string\"),\n",
    "     (\"duration\", \"float\"),\n",
    "     (\"days_left\", \"int\"),\n",
    "     (\"price\", \"int\")\n",
    "     ]\n",
    "airlines_schema = SparkUtils.generate_schema(airlines_schema_columns)\n",
    "airlines_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24383fa-5efe-4afd-9c83-4f74712e2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines = spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .schema(airlines_schema) \\\n",
    "                .csv(\"/opt/spark/work-dir/data/Airline/\")\n",
    "\n",
    "df_airlines.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534eb92-1761-4b97-877c-0be8a351fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col, count, isnull, when\n",
    "print(f\"number of records before cleaning: {df_airlines.count()}\")\n",
    "# Get number of null values for each column before cleaning \n",
    "df_airlines.select([count(when(isnull(c[0]) | col(c[0]).isNull(), c[0])).alias(c[0]) for c in airlines_schema_columns]).show()\n",
    "\n",
    "# Perform data cleaning with trim (column by column)\n",
    "airlines_clean = df_airlines \\\n",
    "        .dropDuplicates([\"index\"]) \\\n",
    "        .withColumn(\"airline\", trim(\"airline\")) \\\n",
    "        .withColumn(\"source_city\", trim(\"source_city\")) \\\n",
    "        .withColumn(\"destination_city\", trim(\"destination_city\")) \\\n",
    "        .filter(col(\"price\").isNotNull())\n",
    "\n",
    "# Simply using dropna()\n",
    "airlines_clean_v2 = df_airlines.dropna()\n",
    "\n",
    "print(f\"number of records after cleaning with trim: {airlines_clean.count()}\")\n",
    "airlines_clean.select([count(when(isnull(c[0]) | col(c[0]).isNull(), c[0])).alias(c[0]) for c in airlines_schema_columns]).show()\n",
    "\n",
    "print(f\"number of records after cleaning with dropna: {airlines_clean_v2.count()}\")\n",
    "airlines_clean_v2.select([count(when(isnull(c[0]) | col(c[0]).isNull(), c[0])).alias(c[0]) for c in airlines_schema_columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, concat_ws, avg, min, max, count\n",
    "\n",
    "# Crear sesión Spark\n",
    "spark = SparkSession.builder.appName(\"Lab03_DataCleaning\").getOrCreate()\n",
    "\n",
    "# Cargar dataset (ajusta la ruta según donde lo tengas guardado)\n",
    "df = spark.read.csv(\"airlines.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Vista inicial\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar nulos antes\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Eliminar columnas innecesarias (ejemplo: 'flight', 'additional_info')\n",
    "df_clean = df.drop(\"flight\", \"additional_info\")\n",
    "\n",
    "# Contar nulos después\n",
    "df_clean.select([count(when(col(c).isNull(), c)).alias(c) for c in df_clean.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar stops\n",
    "df_clean = df_clean.withColumn(\"stops\", \n",
    "    when(col(\"stops\") == \"zero\", 0)\n",
    "    .when(col(\"stops\") == \"one\", 1)\n",
    "    .when(col(\"stops\") == \"two_or_more\", 2)\n",
    "    .otherwise(col(\"stops\"))\n",
    ")\n",
    "\n",
    "# Crear route\n",
    "df_clean = df_clean.withColumn(\"route\", concat_ws(\" → \", col(\"source_city\"), col(\"destination_city\")))\n",
    "\n",
    "# Categorizar departure_time\n",
    "df_clean = df_clean.withColumn(\"departure_cat\",\n",
    "    when(col(\"departure_time\").isin(\"Early_Morning\"), 0)\n",
    "    .when(col(\"departure_time\").isin(\"Morning\"), 1)\n",
    "    .when(col(\"departure_time\").isin(\"Afternoon\"), 2)\n",
    "    .when(col(\"departure_time\").isin(\"Evening\"), 3)\n",
    "    .when(col(\"departure_time\").isin(\"Night\"), 4)\n",
    ")\n",
    "\n",
    "# Categorizar arrival_time\n",
    "df_clean = df_clean.withColumn(\"arrival_cat\",\n",
    "    when(col(\"arrival_time\").isin(\"Early_Morning\"), 0)\n",
    "    .when(col(\"arrival_time\").isin(\"Morning\"), 1)\n",
    "    .when(col(\"arrival_time\").isin(\"Afternoon\"), 2)\n",
    "    .when(col(\"arrival_time\").isin(\"Evening\"), 3)\n",
    "    .when(col(\"arrival_time\").isin(\"Night\"), 4)\n",
    ")\n",
    "\n",
    "# Columna is_expensive\n",
    "df_clean = df_clean.withColumn(\"is_expensive\", when(col(\"price\") > 6000, True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad53e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promedio de precio por aerolínea\n",
    "df_clean.groupBy(\"airline\").agg(avg(\"price\").alias(\"avg_price\")).show()\n",
    "\n",
    "# Duración promedio por ruta\n",
    "df_clean.groupBy(\"route\").agg(avg(\"duration\").alias(\"avg_duration\")).show()\n",
    "\n",
    "# Precio min y max por aerolínea\n",
    "df_clean.groupBy(\"airline\").agg(min(\"price\").alias(\"min_price\"), max(\"price\").alias(\"max_price\")).show()\n",
    "\n",
    "# Conteo de vuelos por categoría de horario de salida\n",
    "df_clean.groupBy(\"departure_cat\").count().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
