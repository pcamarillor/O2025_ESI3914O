{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51a4768-2289-4934-86b4-24f2ae3aa8c1",
   "metadata": {},
   "source": [
    "# Machine Learning algorithm to use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd897280-034b-4e7d-ab1e-84d17452d986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b357f-6c3c-40a8-aa59-db77039f2ae6",
   "metadata": {},
   "source": [
    "# Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e723f6a-1a3d-4d72-9968-72a4efdf1928",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Selected Machine Learning Model\n",
    "**K-means Clustering:** This unsupervised learning algorithm will be used to identify patterns and groupings within the tennis matches. Unlike classification, which predicts a winner, K-means will cluster matches based on similarities in player rankings, betting odds, and tournament types. \n",
    "\n",
    "### 2. Data Source\n",
    "The dataset is obtained from **Kaggle** and is titled **\"ATP Tennis 2000 - 2025 Daily update\"**. It is an open-source repository maintained by the user *dissfya* containing daily updated match results from the Association of Tennis Professionals (ATP).\n",
    "\n",
    "* **URL:** [ATP Tennis 2000 - 2025 Daily update](https://www.kaggle.com/datasets/dissfya/atp-tennis-2000-2023daily-pull/data)\n",
    "\n",
    "### 3. Size of the Dataset\n",
    "The dataset contains a comprehensive history of professional tennis matches spanning over 25 years.\n",
    "\n",
    "* **Volume (Rows):** The dataset contains approximately **65,884 records**, where each row represents a unique professional tennis match played between 2000 and 2025.\n",
    "* **Total Features (Columns):** There are **17 columns** in the raw dataset, including categorical details (Tournament, Surface) and numerical stats (Ranks, Odds).\n",
    "\n",
    "### 4. Clustering Dimensions\n",
    "For K-means clustering, the algorithm requires numerical input vectors. The \"dimensions\" of the clustering problem correspond to the number of numerical features selected for the model.\n",
    "\n",
    "* **Number of Dimensions:** I will initially use **4 to 6 dimensions** based on the available numerical features.\n",
    "* **Selected Features:**\n",
    "    1.  `Rank_1`: Ranking of Player 1.\n",
    "    2.  `Rank_2`: Ranking of Player 2.\n",
    "    3.  `Odd_1`: Betting odds for Player 1.\n",
    "    4.  `Odd_2`: Betting odds for Player 2.\n",
    "    5.  `Pts_1`: ATP Points for Player 1.\n",
    "    6.  `Pts_2`: ATP Points for Player 2.\n",
    "\n",
    "Since K-means calculates Euclidean distance, these dimensions will need to be scaled (normalized) so that larger numbers (like ATP Points) do not dominate smaller numbers (like Odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5239a-d1c8-4c60-b307-ceedf0c3638f",
   "metadata": {},
   "source": [
    "# ML Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f9807d4-0e38-408a-a2d2-86aadc55c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML: K-means\") \\\n",
    "    .master(\"spark://390030c017e5:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Optimization (reduce the number of shuffle partitions)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8435073-5d43-4ce5-a53c-841e32527b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "+------+-----+\n",
      "|Rank_1|Odd_1|\n",
      "+------+-----+\n",
      "|  63.0| -1.0|\n",
      "|  56.0| -1.0|\n",
      "|  40.0| -1.0|\n",
      "|  87.0| -1.0|\n",
      "|  81.0| -1.0|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from codrenatat.spark_utils import SparkUtils\n",
    "\n",
    "# Define Schema\n",
    "columns_types = [\n",
    "    (\"Tournament\", \"string\"),\n",
    "    (\"Date\", \"string\"),\n",
    "    (\"Series\", \"string\"),\n",
    "    (\"Court\", \"string\"),\n",
    "    (\"Surface\", \"string\"),\n",
    "    (\"Round\", \"string\"),\n",
    "    (\"Best_of\", \"int\"),\n",
    "    (\"Player_1\", \"string\"),\n",
    "    (\"Player_2\", \"string\"),\n",
    "    (\"Winner\", \"string\"),\n",
    "    (\"Rank_1\", \"float\"),   \n",
    "    (\"Rank_2\", \"float\"),   \n",
    "    (\"Pts_1\", \"float\"),    \n",
    "    (\"Pts_2\", \"float\"),    \n",
    "    (\"Odd_1\", \"float\"),   \n",
    "    (\"Odd_2\", \"float\"),    \n",
    "    (\"Score\", \"string\")\n",
    "]\n",
    "\n",
    "# Generate the Schema\n",
    "atp_schema = SparkUtils.generate_schema(columns_types)\n",
    "\n",
    "# Load the DataFrame\n",
    "atp_df = spark \\\n",
    "    .read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(atp_schema) \\\n",
    "    .csv(\"/opt/spark/work-dir/data/mlproject/atp_tennis.csv\")\n",
    "\n",
    "# Verify the data is actually there now\n",
    "print(\"Data loaded successfully.\")\n",
    "atp_df.select(\"Rank_1\", \"Odd_1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2847203d-6033-41c1-9fb7-13c114bc2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows ready for scaling: 66681\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "\n",
    "# Define the numerical input columns\n",
    "input_cols = [\"Rank_1\", \"Rank_2\", \"Odd_1\", \"Odd_2\", \"Pts_1\", \"Pts_2\"]\n",
    "\n",
    "# We use the imputation to help us fill the missing values with the mean\n",
    "imputer = Imputer(\n",
    "    inputCols=input_cols, \n",
    "    outputCols=input_cols, \n",
    "    strategy='mean'\n",
    ")\n",
    "# Learn the means from the data and fill the nulls\n",
    "imputer_model = imputer.fit(atp_df)\n",
    "imputed_df = imputer_model.transform(atp_df)\n",
    "\n",
    "# This assembler helps us combine the filled columns into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "assembled_df = assembler.transform(imputed_df)\n",
    "\n",
    "print(f\"Rows ready for scaling: {assembled_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ddea216-78c7-4c0b-a812-09bfea54c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Scaler\n",
    "# We take the features column we just made and create scaled_features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Fit and Transform\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "scaled_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "print(\"Data scaled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b82a07f-18d4-4021-b166-b34fec0c9f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training K-means for different k values ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 01:01:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/21 01:01:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2 processed. Score: 0.7295607160307094\n",
      "k=4 processed. Score: 0.7354107932917343\n",
      "k=6 processed. Score: 0.4026921847770947\n",
      "k=8 processed. Score: 0.38662504932465297\n",
      "k=10 processed. Score: 0.3602413046042269\n",
      "\n",
      "Silhouette scores by k:\n",
      " k=4  silhouette=0.73541\n",
      " k=2  silhouette=0.72956\n",
      " k=6  silhouette=0.40269\n",
      " k=8  silhouette=0.38663\n",
      " k=10 silhouette=0.36024\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Define the list of k values to test\n",
    "ks = [2, 4, 6, 8, 10] \n",
    "results = []\n",
    "\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"scaled_features\") \n",
    "\n",
    "print(\"--- Training K-means for different k values ---\")\n",
    "\n",
    "for kk in ks: \n",
    "    kmeans = KMeans().setK(kk).setSeed(13).setFeaturesCol(\"scaled_features\")\n",
    "    \n",
    "    # Train using the scaled dataframe\n",
    "    model = kmeans.fit(scaled_df) \n",
    "    \n",
    "    # Predict on the scaled dataframe\n",
    "    predictions = model.transform(scaled_df) \n",
    "    \n",
    "    score = evaluator.evaluate(predictions)\n",
    "    results.append((kk, float(score), model)) \n",
    "    \n",
    "    print(f\"k={kk} processed. Score: {score}\")\n",
    "\n",
    "# Sort and Print Results\n",
    "results_sorted = sorted(results, key=lambda t: t[1], reverse=True) \n",
    "print(\"\\nSilhouette scores by k:\") \n",
    "for kk, sc, m in results_sorted: \n",
    "    print(f\" k={kk:<2} silhouette={sc:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58675608-79d5-41d7-9c9f-68240b8b916e",
   "metadata": {},
   "source": [
    "# ML Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff2fae5a-d425-4b6d-a5ba-af35263f9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best Result ---\n",
      "The optimal number of clusters is k=4\n",
      "Silhouette Score: 0.73541\n",
      "Cluster Centers: \n",
      "[0.65496421 0.7807155  0.75741075 0.79515759 0.4684377  0.44794199]\n",
      "[0.61556311 0.0469596  3.34273403 0.44018429 0.92290878 3.84581422]\n",
      "[4.74978283 0.944666   1.55171488 0.47289305 0.04894032 0.40779585]\n",
      "[0.05593583 0.83834462 0.42914153 3.50712834 3.7529437  0.8047987 ]\n"
     ]
    }
   ],
   "source": [
    "# Extract the best model from the sorted results (Index 0 is the highest score)\n",
    "best_k, best_score, best_model = results_sorted[0]\n",
    "\n",
    "print(f\"--- Best Result ---\")\n",
    "print(f\"The optimal number of clusters is k={best_k}\")\n",
    "print(f\"Silhouette Score: {best_score:.5f}\")\n",
    "\n",
    "# Display the Cluster Centers for the best model\n",
    "print(\"Cluster Centers: \")\n",
    "for center in best_model.clusterCenters():\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8802825-5c8d-4450-ba0d-e5495406c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_path = \"/opt/spark/work-dir/data/mlproject/kmeans_atp_model\"\n",
    "best_model.write().overwrite().save(model_path)\n",
    "\n",
    "print(f\"Best model (k={best_k}) saved successfully to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a306b4-d64a-4673-8544-874162156a46",
   "metadata": {},
   "source": [
    "I evaluated the performance of my model using the **Silhouette Score**, which measures how similar a data point is to its own cluster compared to other clusters.  \n",
    "The score ranges from **-1 to 1**, where values closer to **1** indicate dense, well-separated clusters, which is what we want.\n",
    "\n",
    "To generate the predictions, I applied the trained model using the `.transform()` method on my dataframe. This appended a new column named **prediction**, containing the **cluster ID** for every match in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "After testing values of `k = 2, 4, 6, 8, 10`, the evaluation produced the following Silhouette Scores:\n",
    "\n",
    "| k  | Silhouette Score |\n",
    "|----|------------------|\n",
    "| 4  | **0.73541** |\n",
    "| 2  | 0.72956 |\n",
    "| 6  | 0.40269 |\n",
    "| 8  | 0.38663 |\n",
    "| 10 | 0.36024 |\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "The model achieved its highest Silhouette Score of **0.735** with **k = 4**, indicating that the tennis matches naturally fall into **four distinct categories**.\n",
    "\n",
    "The sharp drop in the score at **k = 6 (0.40)** suggests that forcing the data into more than four groups leads to **overlapping, poorly defined clusters**.\n",
    "\n",
    "**Therefore, 4 is the optimal number of clusters for this dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6956c9d-eb16-490f-a74f-f7aedeab289d",
   "metadata": {},
   "source": [
    "# What Is My Model Doing With This Data?\n",
    "\n",
    "My model analyzed **6 dimensions** in the data:  \n",
    "**Ranks, Odds, and Points for both players**.  \n",
    "It then grouped the matches into **4 distinct clusters (k = 4)** based on mathematical similarity.\n",
    "\n",
    "Since I used **StandardScaler**, the *Cluster Centers* represent **z-scores** (how many standard deviations above or below the mean a value is):\n",
    "\n",
    "- **Positive value** → higher than average  \n",
    "- **Negative value** → lower than average  \n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation of the 4 Clusters\n",
    "\n",
    "Based on the cluster centers in the notebook, the model likely identified the following patterns:\n",
    "\n",
    "### **Cluster A – Player 1 Dominant**\n",
    "Matches where:\n",
    "- Player 1 has **very high points**  \n",
    "- Player 2 has **very high odds** (meaning Player 2 is the underdog)\n",
    "\n",
    "### **Cluster B – Player 2 Dominant**\n",
    "Matches where:\n",
    "- Player 2 has **high points**  \n",
    "- Player 1 has **high odds**  \n",
    "\n",
    "### **Cluster C – Competitive / Balanced Matches**\n",
    "- Ranks and odds are close to **0 (average)**  \n",
    "- Players are **evenly matched**  \n",
    "- Indicates standard, competitive matches\n",
    "\n",
    "### **Cluster D – Low-Tier / Qualifier Matches**\n",
    "- Both players have **high Rank numbers** (e.g., 100+, 200+)  \n",
    "- Suggests **lower-tier players** or **qualifying-round matches**\n",
    "\n",
    "---\n",
    "\n",
    "These clusters help us understand the different **types of tennis matches** that naturally emerge from the data based on performance and ranking characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241fb5ff-66a5-4b47-a9a4-68b704392316",
   "metadata": {},
   "source": [
    "## Configuration and Pipeline\n",
    "\n",
    "To prepare the data for the K-means algorithm, I implemented a **PySpark Pipeline** consisting of three stages:\n",
    "\n",
    "### **1. Imputation**\n",
    "I used an **Imputer** with the strategy `\"mean\"` to fill missing values in the **Ranks** and **Odds** columns.  \n",
    "This prevented the model from dropping rows with incomplete data and ensured consistent input for clustering.\n",
    "\n",
    "### **2. Vector Assembly**\n",
    "I combined the six numerical features—  \n",
    "**Rank_1, Rank_2, Odd_1, Odd_2, Pts_1, Pts_2** —  \n",
    "into a single feature vector using `VectorAssembler`.\n",
    "\n",
    "### **3. Feature Scaling**\n",
    "I applied **StandardScaler** to normalize the dataset.  \n",
    "This step is essential because **K-means relies on Euclidean distance**:\n",
    "\n",
    "- **ATP Points** can range into the thousands  \n",
    "- **Odds** typically range from 1.0 to 20.0  \n",
    "\n",
    "Without scaling, ATP Points would dominate distance calculations, biasing the clustering results. Standardization ensures all features contribute proportionally.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameters and Justification\n",
    "\n",
    "I configured the **KMeans** estimator with the following hyperparameters:\n",
    "\n",
    "### **k (Number of Clusters)**\n",
    "I treated **k** as a tunable hyperparameter and tested values **2, 4, 6, 8, and 10**.  \n",
    "This allowed me to determine which setting best captured the natural structure of the data.\n",
    "\n",
    "### **seed (Random Seed)**\n",
    "I set `seed = 13`.  \n",
    "Since K-means initializes cluster centers randomly, fixing the seed ensures **reproducible and consistent results** across runs.\n",
    "\n",
    "### **featuresCol**\n",
    "I set `featuresCol = \"scaled_features\"` so the model trains on the **normalized feature vector** rather than the raw inputs.\n",
    "\n",
    "---\n",
    "\n",
    "These configuration choices help ensure the model is both **mathematically sound** and **reproducible**, while producing clusters that accurately reflect patterns in the tennis match data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aef7cf-fd5c-458a-a0fc-286ff2b0a6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
