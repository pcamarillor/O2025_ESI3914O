{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d9622",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electr칩nica, Sistemas e Inform치tica** </center>\n",
    "---\n",
    "## <center> Computer Systems Engineering  </center>\n",
    "---\n",
    "### <center> Big Data Processing </center>\n",
    "---\n",
    "#### <center> **Autumn 2025** </center>\n",
    "\n",
    "#### <center> **Final Project: Batch Processing** </center>\n",
    "---\n",
    "\n",
    "**Date**: October, 2025\n",
    "\n",
    "**Student Name**: Francisco De Jesus Delgado Carrasco\n",
    "\n",
    "**Professor**: Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd2743",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this project, I will work with the **\"Historical Military Battles\"** dataset ([Kaggle](https://www.kaggle.com/datasets/residentmario/database-of-battles?resource=download)), which contains information about battles from **1600 AD to 1973 AD**. \n",
    "\n",
    "The goal is to develop a **data pipeline** to **identify the most influential factors determining the outcomes** of these historical battles.\n",
    "\n",
    "We will use **batch processing** to clean and analyze the historical data. The processed data will be stored in a **relational database** to enable the analysis of correlations between key variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b962",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The adopted model is **relational**, centered on the `battles.csv` table as the main entity, linked to auxiliary tables describing terrain, weather, actors, and commanders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82306039-c1ff-47d5-b9e4-1aba018a3782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/28 05:33:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/28 05:33:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- isqno: integer (nullable = true)\n",
      " |-- war: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- locn: string (nullable = true)\n",
      " |-- campgn: string (nullable = true)\n",
      " |-- postype: integer (nullable = true)\n",
      " |-- post1: string (nullable = true)\n",
      " |-- post2: string (nullable = true)\n",
      " |-- front: integer (nullable = true)\n",
      " |-- depth: integer (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      " |-- aeroa: integer (nullable = true)\n",
      " |-- surpa: integer (nullable = true)\n",
      " |-- cea: integer (nullable = true)\n",
      " |-- leada: integer (nullable = true)\n",
      " |-- trnga: integer (nullable = true)\n",
      " |-- morala: integer (nullable = true)\n",
      " |-- logsa: integer (nullable = true)\n",
      " |-- momnta: integer (nullable = true)\n",
      " |-- intela: integer (nullable = true)\n",
      " |-- techa: integer (nullable = true)\n",
      " |-- inita: integer (nullable = true)\n",
      " |-- wina: integer (nullable = true)\n",
      " |-- kmda: double (nullable = true)\n",
      " |-- crit: integer (nullable = true)\n",
      " |-- quala: integer (nullable = true)\n",
      " |-- resa: integer (nullable = true)\n",
      " |-- mobila: integer (nullable = true)\n",
      " |-- aira: integer (nullable = true)\n",
      " |-- fprepa: integer (nullable = true)\n",
      " |-- wxa: integer (nullable = true)\n",
      " |-- terra: integer (nullable = true)\n",
      " |-- leadaa: integer (nullable = true)\n",
      " |-- plana: integer (nullable = true)\n",
      " |-- surpaa: integer (nullable = true)\n",
      " |-- mana: integer (nullable = true)\n",
      " |-- logsaa: integer (nullable = true)\n",
      " |-- fortsa: integer (nullable = true)\n",
      " |-- deepa: integer (nullable = true)\n",
      " |-- is_hero: integer (nullable = true)\n",
      " |-- war2: string (nullable = true)\n",
      " |-- war3: string (nullable = true)\n",
      " |-- war4: string (nullable = true)\n",
      " |-- war4_theater: string (nullable = true)\n",
      " |-- dbpedia: string (nullable = true)\n",
      " |-- cow_warno: integer (nullable = true)\n",
      " |-- cow_warname: string (nullable = true)\n",
      " |-- war_initiator: integer (nullable = true)\n",
      " |-- parent: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 05:33:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------+--------------+----------------+----------------------------------+-------+-----+-----+-----+-----+----+-----+-----+----+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+--------------------------------+--------------------------------+------------------------------+------------+----------------------------------------------------+---------+-----------+-------------+------+\n",
      "|isqno|war                             |name          |locn            |campgn                            |postype|post1|post2|front|depth|time|aeroa|surpa|cea |leada|trnga|morala|logsa|momnta|intela|techa|inita|wina|kmda|crit|quala|resa|mobila|aira|fprepa|wxa|terra|leadaa|plana|surpaa|mana|logsaa|fortsa|deepa|is_hero|war2                            |war3                            |war4                          |war4_theater|dbpedia                                             |cow_warno|cow_warname|war_initiator|parent|\n",
      "+-----+--------------------------------+--------------+----------------+----------------------------------+-------+-----+-----+-----+-----+----+-----+-----+----+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+--------------------------------+--------------------------------+------------------------------+------------+----------------------------------------------------+---------+-----------+-------------+------+\n",
      "|1    |NETHERLAND'S WAR OF INDEPENDENCE|NIEUPORT      |SPANISH FLANDERS|NIEUPORT 1600                     |0      |HD   |NULL |0    |0    |0   |0    |0    |NULL|NULL |0    |0     |0    |1     |0     |0    |1    |NULL|NULL|1   |NULL |NULL|0     |0   |0     |0  |0    |NULL  |0    |0     |0   |0     |0     |0    |1      |NETHERLAND'S WAR OF INDEPENDENCE|NETHERLAND'S WAR OF INDEPENDENCE|Eighty Years War of 1568-1648 |NULL        |http://dbpedia.org/resource/Battle_of_Nieuwpoort    |NULL     |NULL       |0            |NULL  |\n",
      "|2    |THIRTY YEAR'S WAR               |WHITE MOUNTAIN|BOHEMIA         |BOHEMIA 1620                      |1      |HD   |PD   |1    |0    |0   |0    |0    |0   |0    |0    |0     |0    |1     |0     |0    |1    |1   |1.0 |1   |1    |1   |0     |0   |0     |0  |NULL |1     |0    |0     |0   |0     |NULL  |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_White_Mountain|NULL     |NULL       |0            |NULL  |\n",
      "|3    |THIRTY YEAR'S WAR               |WIMPFEN       |PALATINATE      |PALATINATE 1622                   |0      |HD   |NULL |0    |0    |0   |0    |0    |0   |1    |0    |0     |0    |NULL  |0     |0    |0    |1   |1.0 |1   |0    |0   |0     |0   |0     |0  |0    |1     |0    |0     |0   |0     |0     |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_Wimpfen       |NULL     |NULL       |0            |NULL  |\n",
      "|4    |THIRTY YEAR'S WAR               |DESSAU BRIDGE |ANHALT          |DANISH INVASION OF GERMANY 1625-26|0      |PD   |NULL |0    |0    |0   |0    |NULL |0   |NULL |0    |0     |0    |0     |NULL  |0    |NULL |NULL|NULL|1   |0    |NULL|0     |0   |NULL  |0  |0    |NULL  |NULL |NULL  |0   |0     |NULL  |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_Dessau_Bridge |NULL     |NULL       |1            |NULL  |\n",
      "|5    |THIRTY YEAR'S WAR               |LUTTER        |BRUNSWICK       |DANISH INVASION OF GERMANY 1625-26|0      |HD   |NULL |0    |0    |0   |0    |0    |0   |1    |0    |1     |0    |1     |0     |0    |0    |1   |3.0 |1   |1    |0   |0     |0   |0     |0  |0    |1     |0    |0     |0   |0     |0     |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_Lutter        |NULL     |NULL       |0            |NULL  |\n",
      "+-----+--------------------------------+--------------+----------------+----------------------------------+-------+-----+-----+-----+-----+----+-----+-----+----+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+--------------------------------+--------------------------------+------------------------------+------------+----------------------------------------------------+---------+-----------+-------------+------+\n",
      "only showing top 5 rows\n",
      "Total de batallas registradas: 660\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Historical Battles Batch Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load main battles dataset\n",
    "battles_df = spark.read.csv(\"../../data/HistoricalBattles/battles.csv\", header=True, inferSchema=True)\n",
    "\n",
    "battles_df.printSchema()\n",
    "battles_df.show(5, truncate=False)\n",
    "\n",
    "print(f\"Total de batallas registradas: {battles_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrain Schema:\n",
      "root\n",
      " |-- isqno: integer (nullable = true)\n",
      " |-- terrano: integer (nullable = true)\n",
      " |-- terra1: string (nullable = true)\n",
      " |-- terra2: string (nullable = true)\n",
      " |-- terra3: string (nullable = true)\n",
      "\n",
      "\n",
      "Weather Schema:\n",
      "root\n",
      " |-- isqno: integer (nullable = true)\n",
      " |-- wxno: integer (nullable = true)\n",
      " |-- wx1: string (nullable = true)\n",
      " |-- wx2: string (nullable = true)\n",
      " |-- wx3: string (nullable = true)\n",
      " |-- wx4: string (nullable = true)\n",
      " |-- wx5: string (nullable = true)\n",
      "\n",
      "\n",
      "Actors Schema:\n",
      "root\n",
      " |-- isqno: integer (nullable = true)\n",
      " |-- attacker: integer (nullable = true)\n",
      " |-- n: integer (nullable = true)\n",
      " |-- actor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load auxiliary datasets\n",
    "terrain_df = spark.read.csv(\"../../data/HistoricalBattles/terrain.csv\", header=True, inferSchema=True)\n",
    "weather_df = spark.read.csv(\"../../data/HistoricalBattles/weather.csv\", header=True, inferSchema=True)\n",
    "actors_df = spark.read.csv(\"../../data/HistoricalBattles/battle_actors.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Terrain Schema:\")\n",
    "terrain_df.printSchema()\n",
    "print(\"\\nWeather Schema:\")\n",
    "weather_df.printSchema()\n",
    "print(\"\\nActors Schema:\")\n",
    "actors_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a0e72",
   "metadata": {},
   "source": [
    "# Transformations and Actions\n",
    "\n",
    "In this section, we perform the following transformations:\n",
    "\n",
    "1. **Data Cleaning**: Handle missing values and filter incomplete records\n",
    "2. **Feature Engineering**: Create derived columns (`strategic_factor`, `battle_complexity`, `century`)\n",
    "3. **Joins**: Combine battles with terrain, weather, and actors information\n",
    "4. **Aggregations**: Analyze patterns by century, war type, and strategic factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean1",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19657e70-f413-4eed-9050-2374dfe32686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in battles dataset:\n",
      "+-----+---+----+----+------+-------+-----+-----+-----+-----+----+-----+-----+---+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+----+----+----+------------+-------+---------+-----------+-------------+------+\n",
      "|isqno|war|name|locn|campgn|postype|post1|post2|front|depth|time|aeroa|surpa|cea|leada|trnga|morala|logsa|momnta|intela|techa|inita|wina|kmda|crit|quala|resa|mobila|aira|fprepa|wxa|terra|leadaa|plana|surpaa|mana|logsaa|fortsa|deepa|is_hero|war2|war3|war4|war4_theater|dbpedia|cow_warno|cow_warname|war_initiator|parent|\n",
      "+-----+---+----+----+------+-------+-----+-----+-----+-----+----+-----+-----+---+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+----+----+----+------------+-------+---------+-----------+-------------+------+\n",
      "|    0|  0|   0|   0|     0|      0|    2|  536|   47|   47|  47|   92|   15|117|  135|  137|    45|   58|    39|    82|   41|   59| 219|  69|   0|  112| 140|    52|  65|    86| 99|  321|   140|   90|    49|  64|    51|   369|  199|      0|   1|   1|   0|         353|    246|      150|        150|            0|   624|\n",
      "+-----+---+----+----+------+-------+-----+-----+-----+-----+----+-----+-----+---+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+----+----+----+------------+-------+---------+-----------+-------------+------+\n",
      "\n",
      "Records after cleaning: 660 (from 660)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, isnan, count, round as spark_round\n",
    "\n",
    "# Check for missing values in battles dataset\n",
    "print(\"Missing values in battles dataset:\")\n",
    "battles_df.select([count(when(col(c).isNull(), c)).alias(c) for c in battles_df.columns]).show()\n",
    "\n",
    "# Remove records with missing critical fields\n",
    "battles_clean = battles_df.filter(\n",
    "    col(\"name\").isNotNull() &\n",
    "    col(\"isqno\").isNotNull() &\n",
    "    col(\"campgn\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Records after cleaning: {battles_clean.count()} (from {battles_df.count()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature1",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feature2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A침os extra칤dos (primeros 10 registros):\n",
      "+----------------------------------+----+\n",
      "|campgn                            |year|\n",
      "+----------------------------------+----+\n",
      "|NIEUPORT 1600                     |1600|\n",
      "|BOHEMIA 1620                      |1620|\n",
      "|PALATINATE 1622                   |1622|\n",
      "|DANISH INVASION OF GERMANY 1625-26|1625|\n",
      "|DANISH INVASION OF GERMANY 1625-26|1625|\n",
      "|LEIPZIG 1631                      |1631|\n",
      "|BAVARIA 1632                      |1632|\n",
      "|NUREMBERG 1632                    |1632|\n",
      "|SAXONY 1632                       |1632|\n",
      "|BAVARIA 1634                      |1634|\n",
      "+----------------------------------+----+\n",
      "only showing top 10 rows\n",
      "+--------------+----------------------------------+----+-------+----------------+-----------------+\n",
      "|name          |campgn                            |year|century|strategic_factor|battle_complexity|\n",
      "+--------------+----------------------------------+----+-------+----------------+-----------------+\n",
      "|NIEUPORT      |NIEUPORT 1600                     |1600|17     |0.0             |0.0              |\n",
      "|WHITE MOUNTAIN|BOHEMIA 1620                      |1620|17     |0.0             |0.0              |\n",
      "|WIMPFEN       |PALATINATE 1622                   |1622|17     |0.2             |4.0              |\n",
      "|DESSAU BRIDGE |DANISH INVASION OF GERMANY 1625-26|1625|17     |0.0             |0.0              |\n",
      "|LUTTER        |DANISH INVASION OF GERMANY 1625-26|1625|17     |0.4             |8.0              |\n",
      "|BREITENFELD I |LEIPZIG 1631                      |1631|17     |0.0             |0.0              |\n",
      "|THE LECH      |BAVARIA 1632                      |1632|17     |0.2             |4.0              |\n",
      "|ALTE VESTE    |NUREMBERG 1632                    |1632|17     |0.0             |0.0              |\n",
      "|LUETZEN       |SAXONY 1632                       |1632|17     |0.2             |4.0              |\n",
      "|NORDLINGEN I  |BAVARIA 1634                      |1634|17     |0.2             |4.0              |\n",
      "+--------------+----------------------------------+----+-------+----------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, floor, regexp_extract, coalesce, lit, round as spark_round, when\n",
    "\n",
    "battles_enriched = battles_clean.withColumn(\n",
    "    \"year\",\n",
    "    regexp_extract(col(\"campgn\"), r\"(\\d{4})\", 1).cast(\"int\")\n",
    ")\n",
    "\n",
    "print(\"A침os extra칤dos (primeros 10 registros):\")\n",
    "battles_enriched.select(\"campgn\", \"year\").show(10, truncate=False)\n",
    "\n",
    "battles_enriched = battles_enriched.withColumn(\n",
    "    \"century\",\n",
    "    when(col(\"year\").isNotNull(), floor(col(\"year\") / 100) + 1).otherwise(None)\n",
    ")\n",
    "\n",
    "strategic_columns = [\"leada\", \"morala\", \"logsa\", \"techa\", \"intela\"]\n",
    "\n",
    "for col_name in strategic_columns:\n",
    "    battles_enriched = battles_enriched.withColumn(col_name, coalesce(col(col_name), lit(0)))\n",
    "\n",
    "battles_enriched = battles_enriched.withColumn(\n",
    "    \"strategic_factor\",\n",
    "    spark_round((\n",
    "        col(\"leada\") + col(\"morala\") + col(\"logsa\") + col(\"techa\") + col(\"intela\")\n",
    "    ) / 5, 2)\n",
    ")\n",
    "\n",
    "battles_enriched = battles_enriched.withColumn(\n",
    "    \"battle_complexity\",\n",
    "    spark_round(col(\"strategic_factor\") * 20, 2)\n",
    ")\n",
    "\n",
    "battles_enriched.select(\n",
    "    \"name\", \"campgn\", \"year\", \"century\", \"strategic_factor\", \"battle_complexity\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "join1",
   "metadata": {},
   "source": [
    "## 3. Joins with Auxiliary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "join2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in terrain_df: ['isqno', 'terrano', 'terra1', 'terra2', 'terra3']\n",
      "Columns in weather_df: ['isqno', 'wxno', 'wx1', 'wx2', 'wx3', 'wx4', 'wx5']\n",
      "Columns in actors_df: ['isqno', 'attacker', 'n', 'actor']\n",
      "Combined columns: ['isqno', 'war', 'name', 'locn', 'campgn', 'postype', 'post1', 'post2', 'front', 'depth', 'time', 'aeroa', 'surpa', 'cea', 'leada', 'trnga', 'morala', 'logsa', 'momnta', 'intela'] ...\n",
      "+-----+--------------------------------+--------------+----------------+---------------+-------+-----+-----+-----+-----+----+-----+-----+----+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+--------------------------------+--------------------------------+------------------------------+------------+----------------------------------------------------+---------+-----------+-------------+------+----+-------+----------------+-----------------+-------+------+------+------+----+---+---+---+---+---+--------+---+-----------------+\n",
      "|isqno|war                             |name          |locn            |campgn         |postype|post1|post2|front|depth|time|aeroa|surpa|cea |leada|trnga|morala|logsa|momnta|intela|techa|inita|wina|kmda|crit|quala|resa|mobila|aira|fprepa|wxa|terra|leadaa|plana|surpaa|mana|logsaa|fortsa|deepa|is_hero|war2                            |war3                            |war4                          |war4_theater|dbpedia                                             |cow_warno|cow_warname|war_initiator|parent|year|century|strategic_factor|battle_complexity|terrano|terra1|terra2|terra3|wxno|wx1|wx2|wx3|wx4|wx5|attacker|n  |actor            |\n",
      "+-----+--------------------------------+--------------+----------------+---------------+-------+-----+-----+-----+-----+----+-----+-----+----+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+--------------------------------+--------------------------------+------------------------------+------------+----------------------------------------------------+---------+-----------+-------------+------+----+-------+----------------+-----------------+-------+------+------+------+----+---+---+---+---+---+--------+---+-----------------+\n",
      "|1    |NETHERLAND'S WAR OF INDEPENDENCE|NIEUPORT      |SPANISH FLANDERS|NIEUPORT 1600  |0      |HD   |NULL |0    |0    |0   |0    |0    |NULL|0    |0    |0     |0    |1     |0     |0    |1    |NULL|NULL|1   |NULL |NULL|0     |0   |0     |0  |0    |NULL  |0    |0     |0   |0     |0     |0    |1      |NETHERLAND'S WAR OF INDEPENDENCE|NETHERLAND'S WAR OF INDEPENDENCE|Eighty Years War of 1568-1648 |NULL        |http://dbpedia.org/resource/Battle_of_Nieuwpoort    |NULL     |NULL       |0            |NULL  |1600|17     |0.0             |0.0              |1      |R     |B     |D     |1   |D  |S  |T  |S  |T  |1       |1  |Spain            |\n",
      "|1    |NETHERLAND'S WAR OF INDEPENDENCE|NIEUPORT      |SPANISH FLANDERS|NIEUPORT 1600  |0      |HD   |NULL |0    |0    |0   |0    |0    |NULL|0    |0    |0     |0    |1     |0     |0    |1    |NULL|NULL|1   |NULL |NULL|0     |0   |0     |0  |0    |NULL  |0    |0     |0   |0     |0     |0    |1      |NETHERLAND'S WAR OF INDEPENDENCE|NETHERLAND'S WAR OF INDEPENDENCE|Eighty Years War of 1568-1648 |NULL        |http://dbpedia.org/resource/Battle_of_Nieuwpoort    |NULL     |NULL       |0            |NULL  |1600|17     |0.0             |0.0              |1      |R     |B     |D     |1   |D  |S  |T  |S  |T  |0       |1  |Dutch Republic   |\n",
      "|2    |THIRTY YEAR'S WAR               |WHITE MOUNTAIN|BOHEMIA         |BOHEMIA 1620   |1      |HD   |PD   |1    |0    |0   |0    |0    |0   |0    |0    |0     |0    |1     |0     |0    |1    |1   |1.0 |1   |1    |1   |0     |0   |0     |0  |NULL |1     |0    |0     |0   |0     |NULL  |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_White_Mountain|NULL     |NULL       |0            |NULL  |1620|17     |0.0             |0.0              |1      |R     |M     |NULL  |1   |D  |S  |T  |W  |T  |1       |1  |Holy Roman Empire|\n",
      "|2    |THIRTY YEAR'S WAR               |WHITE MOUNTAIN|BOHEMIA         |BOHEMIA 1620   |1      |HD   |PD   |1    |0    |0   |0    |0    |0   |0    |0    |0     |0    |1     |0     |0    |1    |1   |1.0 |1   |1    |1   |0     |0   |0     |0  |NULL |1     |0    |0     |0   |0     |NULL  |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_White_Mountain|NULL     |NULL       |0            |NULL  |1620|17     |0.0             |0.0              |1      |R     |M     |NULL  |1   |D  |S  |T  |W  |T  |0       |1  |Bohemia          |\n",
      "|3    |THIRTY YEAR'S WAR               |WIMPFEN       |PALATINATE      |PALATINATE 1622|0      |HD   |NULL |0    |0    |0   |0    |0    |0   |1    |0    |0     |0    |NULL  |0     |0    |0    |1   |1.0 |1   |0    |0   |0     |0   |0     |0  |0    |1     |0    |0     |0   |0     |0     |0    |1      |THIRTY YEAR'S WAR               |THIRTY YEAR'S WAR               |Thirty Years' War of 1618-1648|NULL        |http://dbpedia.org/resource/Battle_of_Wimpfen       |NULL     |NULL       |0            |NULL  |1622|17     |0.2             |4.0              |1      |R     |M     |NULL  |1   |D  |S  |H  |$  |T  |1       |2  |Spain            |\n",
      "+-----+--------------------------------+--------------+----------------+---------------+-------+-----+-----+-----+-----+----+-----+-----+----+-----+-----+------+-----+------+------+-----+-----+----+----+----+-----+----+------+----+------+---+-----+------+-----+------+----+------+------+-----+-------+--------------------------------+--------------------------------+------------------------------+------------+----------------------------------------------------+---------+-----------+-------------+------+----+-------+----------------+-----------------+-------+------+------+------+----+---+---+---+---+---+--------+---+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in terrain_df:\", terrain_df.columns)\n",
    "print(\"Columns in weather_df:\", weather_df.columns)\n",
    "print(\"Columns in actors_df:\", actors_df.columns)\n",
    "\n",
    "terrain_df = terrain_df.withColumnRenamed(\"isqno\", \"terrain_isqno\")\n",
    "weather_df = weather_df.withColumnRenamed(\"isqno\", \"weather_isqno\")\n",
    "actors_df = actors_df.withColumnRenamed(\"isqno\", \"actor_isqno\")\n",
    "\n",
    "battles_with_terrain = battles_enriched.join(\n",
    "    terrain_df,\n",
    "    battles_enriched[\"isqno\"] == terrain_df[\"terrain_isqno\"],\n",
    "    \"left\"\n",
    ").drop(\"terrain_isqno\")\n",
    "\n",
    "battles_with_weather = battles_with_terrain.join(\n",
    "    weather_df,\n",
    "    battles_with_terrain[\"isqno\"] == weather_df[\"weather_isqno\"],\n",
    "    \"left\"\n",
    ").drop(\"weather_isqno\")\n",
    "\n",
    "battles_complete = battles_with_weather.join(\n",
    "    actors_df,\n",
    "    battles_with_weather[\"isqno\"] == actors_df[\"actor_isqno\"],\n",
    "    \"left\"\n",
    ").drop(\"actor_isqno\")\n",
    "\n",
    "print(\"Combined columns:\", battles_complete.columns[:20], \"...\")\n",
    "battles_complete.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agg1",
   "metadata": {},
   "source": [
    "## 4. Aggregations and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "agg2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游늵 Battle statistics by century:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 05:40:40 ERROR Executor: Exception in task 0.0 in stage 38.0 (TID 34)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 5 in cell [4]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/28 05:40:40 WARN TaskSetManager: Lost task 0.0 in stage 38.0 (TID 34) (b389927e733a executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"cast\" was called from\n",
      "line 5 in cell [4]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/10/28 05:40:40 ERROR TaskSetManager: Task 0 in stage 38.0 failed 1 times; aborting job\n",
      "{\"ts\": \"2025-10-28 05:40:40.989\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value '' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 5 in cell [4]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o787.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 5 in cell [4]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 5 in cell [4]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m century_stats \u001b[38;5;241m=\u001b[39m battles_valid\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentury\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     33\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_battles\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     34\u001b[0m     spark_round(avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategic_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_strategic_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     35\u001b[0m     spark_round(avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbattle_complexity\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_complexity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m )\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentury\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m游늵 Battle statistics by century:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mcentury_stats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# --- Distribuci칩n de resultados ---\u001b[39;00m\n\u001b[1;32m     42\u001b[0m outcome_distribution \u001b[38;5;241m=\u001b[39m battles_valid\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     43\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     44\u001b[0m     spark_round(avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategic_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_strategic_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/classic/dataframe.py:316\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    309\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    310\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         },\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 5 in cell [4]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agg3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'battles_complete' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create final analytical dataset for persistence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m final_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mbattles_complete\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misqno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentury\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwar\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategic_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbattle_complexity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposleader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposmo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposlog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostech\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposinte\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterraina\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwxdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal dataset schema:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m final_dataset\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'battles_complete' is not defined"
     ]
    }
   ],
   "source": [
    "# Create final analytical dataset for persistence\n",
    "final_dataset = battles_complete.select(\n",
    "    \"isqno\",\n",
    "    \"name\",\n",
    "    \"year\",\n",
    "    \"century\",\n",
    "    \"war\",\n",
    "    \"post\",\n",
    "    \"strategic_factor\",\n",
    "    \"battle_complexity\",\n",
    "    \"posleader\",\n",
    "    \"posmo\",\n",
    "    \"poslog\",\n",
    "    \"postech\",\n",
    "    \"posinte\",\n",
    "    \"terraina\",\n",
    "    \"wxdesc\"\n",
    ")\n",
    "\n",
    "print(\"Final dataset schema:\")\n",
    "final_dataset.printSchema()\n",
    "print(f\"\\nTotal records for persistence: {final_dataset.count()}\")\n",
    "final_dataset.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61d130",
   "metadata": {},
   "source": [
    "# Persistence Data\n",
    "\n",
    "## Database Selection\n",
    "\n",
    "For this project, I selected **PostgreSQL** as the persistence layer based on the following criteria:\n",
    "\n",
    "### Justification:\n",
    "\n",
    "1. **Relational Data Model**: Our dataset follows a relational structure with normalized tables (battles, terrain, weather, actors) that benefit from SQL joins and referential integrity\n",
    "\n",
    "2. **ACID Compliance**: PostgreSQL ensures data consistency and reliability, critical for historical data that shouldn't be modified accidentally\n",
    "\n",
    "3. **Analytical Queries**: PostgreSQL excels at complex aggregations and analytical queries needed for battle outcome analysis\n",
    "\n",
    "4. **Structured Schema**: Our fixed schema with numeric factors (leadership, morale, technology) and categorical data (terrain, weather) maps perfectly to SQL tables\n",
    "\n",
    "5. **Integration with Spark**: Native JDBC support enables seamless data transfer from Spark DataFrames to PostgreSQL tables\n",
    "\n",
    "### Alternative Considered:\n",
    "\n",
    "**MongoDB** (NoSQL) was considered but rejected because:\n",
    "- Our data is highly structured and relational\n",
    "- We need JOIN operations across multiple tables\n",
    "- No need for flexible schema or document storage\n",
    "- SQL aggregations are more intuitive for this use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persist1",
   "metadata": {},
   "source": [
    "## Database Configuration\n",
    "\n",
    "**Note**: Before running the persistence code, ensure PostgreSQL is running and accessible. You can use Docker:\n",
    "\n",
    "```bash\n",
    "docker run --name battles-postgres -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=battles_db -p 5432:5432 -d postgres:15\n",
    "```\n",
    "\n",
    "Or use an existing PostgreSQL instance and update the connection parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69421542-249c-4dda-8ff3-de4a6cb862e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m db_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# In production, use environment variables!\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical_battles_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing to persist \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfinal_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records to PostgreSQL...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Database connection parameters\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/battles_db\"\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",  # In production, use environment variables!\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"historical_battles_analysis\"\n",
    "\n",
    "print(f\"Preparing to persist {final_dataset.count()} records to PostgreSQL...\")\n",
    "print(f\"Target table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persist2",
   "metadata": {},
   "source": [
    "## Write Data to PostgreSQL\n",
    "\n",
    "We use Spark's JDBC writer to persist the processed dataset. The `overwrite` mode ensures idempotency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "persist3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仇 Error persisting data: name 'final_dataset' is not defined\n",
      "\n",
      "Make sure PostgreSQL is running and the JDBC driver is available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Write the final dataset to PostgreSQL\n",
    "    final_dataset.write \\\n",
    "        .jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table_name,\n",
    "            mode=\"overwrite\",  # Use 'append' for incremental loads\n",
    "            properties=db_properties\n",
    "        )\n",
    "    \n",
    "    print(f\"九 Successfully persisted {final_dataset.count()} records to {table_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"仇 Error persisting data: {e}\")\n",
    "    print(\"\\nMake sure PostgreSQL is running and the JDBC driver is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persist4",
   "metadata": {},
   "source": [
    "## Verify Data Persistence\n",
    "\n",
    "Read back a sample of the persisted data to verify the write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "persist5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仇 Error reading data: An error occurred while calling o478.jdbc.\n",
      ": java.lang.ClassNotFoundException: org.postgresql.Driver\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:189)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.jdbc(DataFrameReader.scala:115)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.jdbc(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
      "\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n",
      "\t\tat scala.Option.foreach(Option.scala:437)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 24 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read data back from PostgreSQL\n",
    "    persisted_df = spark.read \\\n",
    "        .jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table_name,\n",
    "            properties=db_properties\n",
    "        )\n",
    "    \n",
    "    print(f\"九 Verification: Read {persisted_df.count()} records from database\")\n",
    "    print(\"\\nSample data from PostgreSQL:\")\n",
    "    persisted_df.select(\"name\", \"year\", \"century\", \"strategic_factor\", \"battle_complexity\").show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"仇 Error reading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persist6",
   "metadata": {},
   "source": [
    "## Save Summary Statistics\n",
    "\n",
    "Additionally, we persist aggregated statistics for quick analytics access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "persist7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "丘멆잺  Error persisting aggregated tables: name 'century_stats' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Persist century statistics\n",
    "    century_stats.write \\\n",
    "        .jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=\"battles_by_century\",\n",
    "            mode=\"overwrite\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "    \n",
    "    print(\"九 Century statistics persisted successfully\")\n",
    "    \n",
    "    # Persist terrain impact analysis\n",
    "    terrain_impact.write \\\n",
    "        .jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=\"battles_by_terrain\",\n",
    "            mode=\"overwrite\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "    \n",
    "    print(\"九 Terrain impact analysis persisted successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"丘멆잺  Error persisting aggregated tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef5f1",
   "metadata": {},
   "source": [
    "# DAG\n",
    "\n",
    "## Spark Execution Plan\n",
    "\n",
    "The Directed Acyclic Graph (DAG) represents the execution plan of our Spark job. To visualize it:\n",
    "\n",
    "1. **Access Spark UI**: While your Spark job is running, navigate to `http://localhost:4040` in your browser\n",
    "2. **Go to SQL Tab**: Click on the \"SQL\" tab to see query executions\n",
    "3. **Select a Query**: Click on one of the completed queries (e.g., the JDBC write operation)\n",
    "4. **View DAG**: Scroll down to see the DAG visualization showing stages and tasks\n",
    "5. **Capture Screenshot**: Take a screenshot of the DAG for documentation\n",
    "\n",
    "### Key Stages in Our Pipeline:\n",
    "\n",
    "1. **Scan CSV**: Reading input files (battles.csv, terrain.csv, weather.csv)\n",
    "2. **Filter**: Data cleaning (removing nulls)\n",
    "3. **Project**: Feature engineering (creating strategic_factor, battle_complexity, century)\n",
    "4. **Join**: Combining battles with terrain and weather data\n",
    "5. **Aggregate**: Computing statistics by century and terrain\n",
    "6. **JDBC Write**: Persisting final dataset to PostgreSQL\n",
    "\n",
    "### Example DAG Screenshot:\n",
    "\n",
    "![Spark DAG](./dag_screenshot.png)\n",
    "\n",
    "*Note: Replace this placeholder with your actual screenshot from the Spark UI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0304f195-981d-4ff1-84c1-ce59b273ef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI URL:\n",
      "http://localhost:4040\n",
      "\n",
      "Navigate to this URL while the Spark session is active to view the DAG.\n",
      "Go to: SQL Tab  Select a query  Scroll to 'DAG Visualization'\n"
     ]
    }
   ],
   "source": [
    "# Print Spark application tracking URL\n",
    "print(\"Spark UI URL:\")\n",
    "print(f\"http://localhost:4040\")\n",
    "print(\"\\nNavigate to this URL while the Spark session is active to view the DAG.\")\n",
    "print(\"Go to: SQL Tab  Select a query  Scroll to 'DAG Visualization'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This batch processing pipeline successfully:\n",
    "\n",
    "1. 九 Loaded and validated historical battle data from multiple CSV sources\n",
    "2. 九 Cleaned and preprocessed data by handling missing values\n",
    "3. 九 Engineered meaningful features (strategic_factor, battle_complexity, century)\n",
    "4. 九 Performed joins across relational tables (battles, terrain, weather)\n",
    "5. 九 Executed aggregations to identify patterns by century and terrain type\n",
    "6. 九 Persisted processed data to PostgreSQL for further analysis\n",
    "7. 九 Documented the Spark DAG execution plan\n",
    "\n",
    "The final dataset enables analysis of the key factors influencing battle outcomes from 1600-1973 AD, including strategic factors, environmental conditions, and temporal trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
